# Gradient-Centralization

## Gradient-Centralization: A New Optimization Technique for Deep Neural Networks

The optimizers are provided in the files: SGD.py, Adam.py and Adagrad.py, including SGD_GC, SGD_GCC, SGDW_GCC, Adam_GC, Adam_GCC, AdamW_GCC and Adagrad_GCC. The optimizers with "_GC" use GC for both Conv layers and FC layers, and the optimizers with "_GCC" use GC only for Conv layers. We can use the following codes to import SGD_GC:
    from SGD import SGD_GC  

### CIFAR100

### Mini-ImageNet

### ImageNet

### Fine-grained Classification

### Objection Detection and Segmentation




